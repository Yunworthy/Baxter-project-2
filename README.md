# Baxter-project-2

![Output sample](https://github.com/zhouyuan7/Baxter-project-2/blob/master/source/hand_baxter.gif)
![Output sample](https://github.com/zhouyuan7/Baxter-project-2/blob/master/source/table_own.gif)


## Introduction

This is my second Baxter project and my first solo Baxter project. This is also the course project of ME 740 Vision, Robotics and Planning in Boston University. The propose of this project is to estimate a spatial position of a tennis ball and a position-velocity movement strategy generated by myself(partly).

This project can also be classified into a visual servoing control method. Compared with my first baxter project, this is an off-line spatial target position estimation and position-veocity Jacobian control. In detail, this project can also be divided into vision and motion parts.

### Vision

For vision part, the pattern recognition is the same as the first project using color separation method. However, the process of transforming the location information from camera image frame to Baxter coordinate is different. In this project we use both two arms' hand cameras and use linear triangulation and single value decomposition method to estimate the target location in Baxter coordinate system. 

![alt text](https://github.com/zhouyuan7/Baxter-project-2/blob/master/source/baxter_vision.png)

### Motion

For motion part, using plain language, now our Baxter opens its 'eyes' and can 'see' something. We want to use its arm to move to it. based on my thought, there are three methods: position control, velocity control and 'position-velocity' control.



![Output sample](https://github.com/zhouyuan7/Baxter-project-2/blob/master/source/table_baxter.gif)

![Output sample](https://github.com/zhouyuan7/Baxter-project-2/blob/master/source/hand_own.gif)

![Output sample](https://github.com/zhouyuan7/Baxter-project-2/blob/master/source/simulation.gif)
